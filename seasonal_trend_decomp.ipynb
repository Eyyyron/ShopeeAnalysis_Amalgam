{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c04366bd",
   "metadata": {},
   "source": [
    "# Seasonal and Trend Decomposition Analysis\n",
    "**Dataset:** Shopee Philippines Uncommon Retail Products (March 2022 – November 2025)  \n",
    "**Objective:** Decompose sales patterns into Trend, Seasonal, and Residual components for product and category-level insights.\n",
    "\n",
    "---\n",
    "\n",
    "## Methodology Overview\n",
    "1. **Data Preparation:** Load CSV, parse dates (dd/mm/yyyy format), select metrics\n",
    "2. **Product-Level Processing:** Truncate pre-listing NaNs, filter by duration (≥12 months)\n",
    "3. **Category-Level Aggregation:** Sum sales by category for market analysis\n",
    "4. **Decomposition:** Apply additive model (Y = T + S + R) with 12-month period\n",
    "5. **Metric Extraction:** Calculate seasonal amplitude, trend slope, residual anomalies\n",
    "6. **Visualization:** Plot strategic samples and export results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedb17c0",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1ebe7e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization styles\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d15df9",
   "metadata": {},
   "source": [
    "## 2. Define Cleanup Function\n",
    "Clean up old export files before generating new results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "664ea711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed old file: seasonal_trend/seasonal_decomposition_results.csv\n",
      "Removed old directory: seasonal_trend/visualizations/\n",
      "Cleanup completed. Ready for new exports.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def cleanup_exports():\n",
    "    \"\"\"\n",
    "    Remove old export files and directories before generating new results.\n",
    "    This ensures we always have fresh exports without accumulating old files.\n",
    "    \"\"\"\n",
    "    # Define export paths\n",
    "    csv_file = 'seasonal_trend/seasonal_decomposition_results.csv'\n",
    "    viz_base_dir = 'seasonal_trend/visualizations'\n",
    "    \n",
    "    # Remove CSV file if it exists\n",
    "    if os.path.exists(csv_file):\n",
    "        os.remove(csv_file)\n",
    "        print(f\"Removed old file: {csv_file}\")\n",
    "    \n",
    "    # Remove visualizations directory if it exists\n",
    "    if os.path.exists(viz_base_dir):\n",
    "        shutil.rmtree(viz_base_dir)\n",
    "        print(f\"Removed old directory: {viz_base_dir}/\")\n",
    "    \n",
    "    print(\"Cleanup completed. Ready for new exports.\")\n",
    "\n",
    "# Execute cleanup\n",
    "cleanup_exports()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8a95d7",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "52be91af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Shape: (1510901, 24)\n",
      "\n",
      "Column names:\n",
      "1. product\n",
      "2. time\n",
      "3. avg.sku_price(₱)\n",
      "4. sold/day\n",
      "5. revenue/day(₱)\n",
      "6. sold/m\n",
      "7. product_sales_rate(%)\n",
      "8. price(₱)\n",
      "9. sku\n",
      "10. sold\n",
      "11. sold/month(₱)\n",
      "12. revenue/month\n",
      "13. new_ratings\n",
      "14. ratings\n",
      "15. ratings_rate\n",
      "16. likes\n",
      "17. rating_star\n",
      "18. new_likes\n",
      "19. id\n",
      "20. top-level_category\n",
      "21. seller_from\n",
      "22. listing_time\n",
      "23. active_months\n",
      "24. suitable_for_seasonal_analysis\n",
      "\n",
      "First few rows:\n",
      "                                             product        time  \\\n",
      "0    One Piece Pearl Cherry Clavicle Necklace Sui...  2022-03-01   \n",
      "1    One Piece Pearl Cherry Clavicle Necklace Sui...  2022-04-01   \n",
      "2    One Piece Pearl Cherry Clavicle Necklace Sui...  2022-05-01   \n",
      "3    One Piece Pearl Cherry Clavicle Necklace Sui...  2022-06-01   \n",
      "4    One Piece Pearl Cherry Clavicle Necklace Sui...  2022-07-01   \n",
      "\n",
      "   avg.sku_price(₱)  sold/day  revenue/day(₱)  sold/m  product_sales_rate(%)  \\\n",
      "0               NaN       NaN             NaN     NaN                    NaN   \n",
      "1               NaN       NaN             NaN     NaN                    NaN   \n",
      "2               NaN       NaN             NaN     NaN                    NaN   \n",
      "3               NaN       NaN             NaN     NaN                    NaN   \n",
      "4               NaN       NaN             NaN     NaN                    NaN   \n",
      "\n",
      "   price(₱)  sku  sold  ...  ratings_rate  likes  rating_star  new_likes  \\\n",
      "0       NaN  NaN   NaN  ...           NaN    NaN          NaN        NaN   \n",
      "1       NaN  NaN   NaN  ...           NaN    NaN          NaN        NaN   \n",
      "2       NaN  NaN   NaN  ...           NaN    NaN          NaN        NaN   \n",
      "3       NaN  NaN   NaN  ...           NaN    NaN          NaN        NaN   \n",
      "4       NaN  NaN   NaN  ...           NaN    NaN          NaN        NaN   \n",
      "\n",
      "            id   top-level_category  seller_from  listing_time  active_months  \\\n",
      "0  18785741873  Fashion Accessories        Local    2023-10-27             25   \n",
      "1  18785741873  Fashion Accessories        Local    2023-10-27             25   \n",
      "2  18785741873  Fashion Accessories        Local    2023-10-27             25   \n",
      "3  18785741873  Fashion Accessories        Local    2023-10-27             25   \n",
      "4  18785741873  Fashion Accessories        Local    2023-10-27             25   \n",
      "\n",
      "  suitable_for_seasonal_analysis  \n",
      "0                           True  \n",
      "1                           True  \n",
      "2                           True  \n",
      "3                           True  \n",
      "4                           True  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "\n",
      "Data types:\n",
      "product                            object\n",
      "time                               object\n",
      "avg.sku_price(₱)                  float64\n",
      "sold/day                          float64\n",
      "revenue/day(₱)                    float64\n",
      "sold/m                            float64\n",
      "product_sales_rate(%)             float64\n",
      "price(₱)                          float64\n",
      "sku                               float64\n",
      "sold                              float64\n",
      "sold/month(₱)                     float64\n",
      "revenue/month                     float64\n",
      "new_ratings                       float64\n",
      "ratings                           float64\n",
      "ratings_rate                      float64\n",
      "likes                             float64\n",
      "rating_star                       float64\n",
      "new_likes                         float64\n",
      "id                                  int64\n",
      "top-level_category                 object\n",
      "seller_from                        object\n",
      "listing_time                       object\n",
      "active_months                       int64\n",
      "suitable_for_seasonal_analysis       bool\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV dataset\n",
    "df = pd.read_csv('consolidated_trimed_20.csv')\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nColumn names:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"{i}. {col}\")\n",
    "\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcda555",
   "metadata": {},
   "source": [
    "## 4. Understand Data Structure\n",
    "Examine the `time` column and identify how the data is organized (already in YYYY-MM-DD format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "20e8eefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 'time' values:\n",
      "0     2022-03-01\n",
      "1     2022-04-01\n",
      "2     2022-05-01\n",
      "3     2022-06-01\n",
      "4     2022-07-01\n",
      "5     2022-08-01\n",
      "6     2022-09-01\n",
      "7     2022-10-01\n",
      "8     2022-11-01\n",
      "9     2022-12-01\n",
      "10    2023-01-01\n",
      "11    2023-02-01\n",
      "12    2023-03-01\n",
      "13    2023-04-01\n",
      "14    2023-05-01\n",
      "15    2023-06-01\n",
      "16    2023-07-01\n",
      "17    2023-08-01\n",
      "18    2023-09-01\n",
      "19    2023-10-01\n",
      "Name: time, dtype: object\n",
      "\n",
      "Unique time values: 45\n",
      "\n",
      "Time value examples:\n",
      "['2022-03-01' '2022-04-01' '2022-05-01' '2022-06-01' '2022-07-01'\n",
      " '2022-08-01' '2022-09-01' '2022-10-01' '2022-11-01' '2022-12-01']\n",
      "\n",
      "Data type of 'time' column: object\n",
      "\n",
      "Checking 'suitable_for_seasonal_analysis' column:\n",
      "suitable_for_seasonal_analysis\n",
      "True     1429787\n",
      "False      81114\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample of key columns:\n",
      "                                              product        time  sold/m  \\\n",
      "0     One Piece Pearl Cherry Clavicle Necklace Sui...  2022-03-01     NaN   \n",
      "1     One Piece Pearl Cherry Clavicle Necklace Sui...  2022-04-01     NaN   \n",
      "2     One Piece Pearl Cherry Clavicle Necklace Sui...  2022-05-01     NaN   \n",
      "3     One Piece Pearl Cherry Clavicle Necklace Sui...  2022-06-01     NaN   \n",
      "4     One Piece Pearl Cherry Clavicle Necklace Sui...  2022-07-01     NaN   \n",
      "5     One Piece Pearl Cherry Clavicle Necklace Sui...  2022-08-01     NaN   \n",
      "6     One Piece Pearl Cherry Clavicle Necklace Sui...  2022-09-01     NaN   \n",
      "7     One Piece Pearl Cherry Clavicle Necklace Sui...  2022-10-01     NaN   \n",
      "8     One Piece Pearl Cherry Clavicle Necklace Sui...  2022-11-01     NaN   \n",
      "9     One Piece Pearl Cherry Clavicle Necklace Sui...  2022-12-01     NaN   \n",
      "10    One Piece Pearl Cherry Clavicle Necklace Sui...  2023-01-01     NaN   \n",
      "11    One Piece Pearl Cherry Clavicle Necklace Sui...  2023-02-01     NaN   \n",
      "12    One Piece Pearl Cherry Clavicle Necklace Sui...  2023-03-01     NaN   \n",
      "13    One Piece Pearl Cherry Clavicle Necklace Sui...  2023-04-01     NaN   \n",
      "14    One Piece Pearl Cherry Clavicle Necklace Sui...  2023-05-01     NaN   \n",
      "15    One Piece Pearl Cherry Clavicle Necklace Sui...  2023-06-01     NaN   \n",
      "16    One Piece Pearl Cherry Clavicle Necklace Sui...  2023-07-01     NaN   \n",
      "17    One Piece Pearl Cherry Clavicle Necklace Sui...  2023-08-01     NaN   \n",
      "18    One Piece Pearl Cherry Clavicle Necklace Sui...  2023-09-01     NaN   \n",
      "19    One Piece Pearl Cherry Clavicle Necklace Sui...  2023-10-01     NaN   \n",
      "\n",
      "    revenue/month   top-level_category  \n",
      "0             NaN  Fashion Accessories  \n",
      "1             NaN  Fashion Accessories  \n",
      "2             NaN  Fashion Accessories  \n",
      "3             NaN  Fashion Accessories  \n",
      "4             NaN  Fashion Accessories  \n",
      "5             NaN  Fashion Accessories  \n",
      "6             NaN  Fashion Accessories  \n",
      "7             NaN  Fashion Accessories  \n",
      "8             NaN  Fashion Accessories  \n",
      "9             NaN  Fashion Accessories  \n",
      "10            NaN  Fashion Accessories  \n",
      "11            NaN  Fashion Accessories  \n",
      "12            NaN  Fashion Accessories  \n",
      "13            NaN  Fashion Accessories  \n",
      "14            NaN  Fashion Accessories  \n",
      "15            NaN  Fashion Accessories  \n",
      "16            NaN  Fashion Accessories  \n",
      "17            NaN  Fashion Accessories  \n",
      "18            NaN  Fashion Accessories  \n",
      "19            NaN  Fashion Accessories  \n",
      "                                              product        time  sold/m  \\\n",
      "0     One Piece Pearl Cherry Clavicle Necklace Sui...  2022-03-01     NaN   \n",
      "1     One Piece Pearl Cherry Clavicle Necklace Sui...  2022-04-01     NaN   \n",
      "2     One Piece Pearl Cherry Clavicle Necklace Sui...  2022-05-01     NaN   \n",
      "3     One Piece Pearl Cherry Clavicle Necklace Sui...  2022-06-01     NaN   \n",
      "4     One Piece Pearl Cherry Clavicle Necklace Sui...  2022-07-01     NaN   \n",
      "5     One Piece Pearl Cherry Clavicle Necklace Sui...  2022-08-01     NaN   \n",
      "6     One Piece Pearl Cherry Clavicle Necklace Sui...  2022-09-01     NaN   \n",
      "7     One Piece Pearl Cherry Clavicle Necklace Sui...  2022-10-01     NaN   \n",
      "8     One Piece Pearl Cherry Clavicle Necklace Sui...  2022-11-01     NaN   \n",
      "9     One Piece Pearl Cherry Clavicle Necklace Sui...  2022-12-01     NaN   \n",
      "10    One Piece Pearl Cherry Clavicle Necklace Sui...  2023-01-01     NaN   \n",
      "11    One Piece Pearl Cherry Clavicle Necklace Sui...  2023-02-01     NaN   \n",
      "12    One Piece Pearl Cherry Clavicle Necklace Sui...  2023-03-01     NaN   \n",
      "13    One Piece Pearl Cherry Clavicle Necklace Sui...  2023-04-01     NaN   \n",
      "14    One Piece Pearl Cherry Clavicle Necklace Sui...  2023-05-01     NaN   \n",
      "15    One Piece Pearl Cherry Clavicle Necklace Sui...  2023-06-01     NaN   \n",
      "16    One Piece Pearl Cherry Clavicle Necklace Sui...  2023-07-01     NaN   \n",
      "17    One Piece Pearl Cherry Clavicle Necklace Sui...  2023-08-01     NaN   \n",
      "18    One Piece Pearl Cherry Clavicle Necklace Sui...  2023-09-01     NaN   \n",
      "19    One Piece Pearl Cherry Clavicle Necklace Sui...  2023-10-01     NaN   \n",
      "\n",
      "    revenue/month   top-level_category  \n",
      "0             NaN  Fashion Accessories  \n",
      "1             NaN  Fashion Accessories  \n",
      "2             NaN  Fashion Accessories  \n",
      "3             NaN  Fashion Accessories  \n",
      "4             NaN  Fashion Accessories  \n",
      "5             NaN  Fashion Accessories  \n",
      "6             NaN  Fashion Accessories  \n",
      "7             NaN  Fashion Accessories  \n",
      "8             NaN  Fashion Accessories  \n",
      "9             NaN  Fashion Accessories  \n",
      "10            NaN  Fashion Accessories  \n",
      "11            NaN  Fashion Accessories  \n",
      "12            NaN  Fashion Accessories  \n",
      "13            NaN  Fashion Accessories  \n",
      "14            NaN  Fashion Accessories  \n",
      "15            NaN  Fashion Accessories  \n",
      "16            NaN  Fashion Accessories  \n",
      "17            NaN  Fashion Accessories  \n",
      "18            NaN  Fashion Accessories  \n",
      "19            NaN  Fashion Accessories  \n"
     ]
    }
   ],
   "source": [
    "# Examine the time column and data structure\n",
    "print(\"Sample 'time' values:\")\n",
    "print(df['time'].head(20))\n",
    "\n",
    "print(f\"\\nUnique time values: {df['time'].nunique()}\")\n",
    "print(f\"\\nTime value examples:\")\n",
    "print(df['time'].unique()[:10])\n",
    "\n",
    "# Check if time is already datetime or needs parsing\n",
    "print(f\"\\nData type of 'time' column: {df['time'].dtype}\")\n",
    "\n",
    "# Check if suitable_for_seasonal_analysis exists\n",
    "if 'suitable_for_seasonal_analysis' in df.columns:\n",
    "    print(f\"\\nChecking 'suitable_for_seasonal_analysis' column:\")\n",
    "    print(df['suitable_for_seasonal_analysis'].value_counts())\n",
    "\n",
    "# Check data structure - is this wide format or long format?\n",
    "print(f\"\\nSample of key columns:\")\n",
    "print(df[['product', 'time', 'sold/m', 'revenue/month', 'top-level_category']].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21666184",
   "metadata": {},
   "source": [
    "## 5. Parse Time Column and Restructure Data\n",
    "Convert time strings to datetime objects and pivot to create time series per product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "192b96c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed dates: 1510901 / 1510901\n",
      "Date range: 2022-03-01 00:00:00 to 2025-11-01 00:00:00\n",
      "\n",
      "Unique periods: 45\n",
      "Sample periods: [Period('2022-03', 'M'), Period('2022-04', 'M'), Period('2022-05', 'M'), Period('2022-06', 'M'), Period('2022-07', 'M'), Period('2022-08', 'M'), Period('2022-09', 'M'), Period('2022-10', 'M'), Period('2022-11', 'M'), Period('2022-12', 'M'), Period('2023-01', 'M'), Period('2023-02', 'M')]\n",
      "\n",
      "Data structure (long format - multiple rows per product):\n",
      "                                              product       date   period  \\\n",
      "0     One Piece Pearl Cherry Clavicle Necklace Sui... 2022-03-01  2022-03   \n",
      "1     One Piece Pearl Cherry Clavicle Necklace Sui... 2022-04-01  2022-04   \n",
      "2     One Piece Pearl Cherry Clavicle Necklace Sui... 2022-05-01  2022-05   \n",
      "3     One Piece Pearl Cherry Clavicle Necklace Sui... 2022-06-01  2022-06   \n",
      "4     One Piece Pearl Cherry Clavicle Necklace Sui... 2022-07-01  2022-07   \n",
      "5     One Piece Pearl Cherry Clavicle Necklace Sui... 2022-08-01  2022-08   \n",
      "6     One Piece Pearl Cherry Clavicle Necklace Sui... 2022-09-01  2022-09   \n",
      "7     One Piece Pearl Cherry Clavicle Necklace Sui... 2022-10-01  2022-10   \n",
      "8     One Piece Pearl Cherry Clavicle Necklace Sui... 2022-11-01  2022-11   \n",
      "9     One Piece Pearl Cherry Clavicle Necklace Sui... 2022-12-01  2022-12   \n",
      "10    One Piece Pearl Cherry Clavicle Necklace Sui... 2023-01-01  2023-01   \n",
      "11    One Piece Pearl Cherry Clavicle Necklace Sui... 2023-02-01  2023-02   \n",
      "12    One Piece Pearl Cherry Clavicle Necklace Sui... 2023-03-01  2023-03   \n",
      "13    One Piece Pearl Cherry Clavicle Necklace Sui... 2023-04-01  2023-04   \n",
      "14    One Piece Pearl Cherry Clavicle Necklace Sui... 2023-05-01  2023-05   \n",
      "\n",
      "    sold/m  revenue/month   top-level_category  \n",
      "0      NaN            NaN  Fashion Accessories  \n",
      "1      NaN            NaN  Fashion Accessories  \n",
      "2      NaN            NaN  Fashion Accessories  \n",
      "3      NaN            NaN  Fashion Accessories  \n",
      "4      NaN            NaN  Fashion Accessories  \n",
      "5      NaN            NaN  Fashion Accessories  \n",
      "6      NaN            NaN  Fashion Accessories  \n",
      "7      NaN            NaN  Fashion Accessories  \n",
      "8      NaN            NaN  Fashion Accessories  \n",
      "9      NaN            NaN  Fashion Accessories  \n",
      "10     NaN            NaN  Fashion Accessories  \n",
      "11     NaN            NaN  Fashion Accessories  \n",
      "12     NaN            NaN  Fashion Accessories  \n",
      "13     NaN            NaN  Fashion Accessories  \n",
      "14     NaN            NaN  Fashion Accessories  \n",
      "\n",
      "Unique periods: 45\n",
      "Sample periods: [Period('2022-03', 'M'), Period('2022-04', 'M'), Period('2022-05', 'M'), Period('2022-06', 'M'), Period('2022-07', 'M'), Period('2022-08', 'M'), Period('2022-09', 'M'), Period('2022-10', 'M'), Period('2022-11', 'M'), Period('2022-12', 'M'), Period('2023-01', 'M'), Period('2023-02', 'M')]\n",
      "\n",
      "Data structure (long format - multiple rows per product):\n",
      "                                              product       date   period  \\\n",
      "0     One Piece Pearl Cherry Clavicle Necklace Sui... 2022-03-01  2022-03   \n",
      "1     One Piece Pearl Cherry Clavicle Necklace Sui... 2022-04-01  2022-04   \n",
      "2     One Piece Pearl Cherry Clavicle Necklace Sui... 2022-05-01  2022-05   \n",
      "3     One Piece Pearl Cherry Clavicle Necklace Sui... 2022-06-01  2022-06   \n",
      "4     One Piece Pearl Cherry Clavicle Necklace Sui... 2022-07-01  2022-07   \n",
      "5     One Piece Pearl Cherry Clavicle Necklace Sui... 2022-08-01  2022-08   \n",
      "6     One Piece Pearl Cherry Clavicle Necklace Sui... 2022-09-01  2022-09   \n",
      "7     One Piece Pearl Cherry Clavicle Necklace Sui... 2022-10-01  2022-10   \n",
      "8     One Piece Pearl Cherry Clavicle Necklace Sui... 2022-11-01  2022-11   \n",
      "9     One Piece Pearl Cherry Clavicle Necklace Sui... 2022-12-01  2022-12   \n",
      "10    One Piece Pearl Cherry Clavicle Necklace Sui... 2023-01-01  2023-01   \n",
      "11    One Piece Pearl Cherry Clavicle Necklace Sui... 2023-02-01  2023-02   \n",
      "12    One Piece Pearl Cherry Clavicle Necklace Sui... 2023-03-01  2023-03   \n",
      "13    One Piece Pearl Cherry Clavicle Necklace Sui... 2023-04-01  2023-04   \n",
      "14    One Piece Pearl Cherry Clavicle Necklace Sui... 2023-05-01  2023-05   \n",
      "\n",
      "    sold/m  revenue/month   top-level_category  \n",
      "0      NaN            NaN  Fashion Accessories  \n",
      "1      NaN            NaN  Fashion Accessories  \n",
      "2      NaN            NaN  Fashion Accessories  \n",
      "3      NaN            NaN  Fashion Accessories  \n",
      "4      NaN            NaN  Fashion Accessories  \n",
      "5      NaN            NaN  Fashion Accessories  \n",
      "6      NaN            NaN  Fashion Accessories  \n",
      "7      NaN            NaN  Fashion Accessories  \n",
      "8      NaN            NaN  Fashion Accessories  \n",
      "9      NaN            NaN  Fashion Accessories  \n",
      "10     NaN            NaN  Fashion Accessories  \n",
      "11     NaN            NaN  Fashion Accessories  \n",
      "12     NaN            NaN  Fashion Accessories  \n",
      "13     NaN            NaN  Fashion Accessories  \n",
      "14     NaN            NaN  Fashion Accessories  \n"
     ]
    }
   ],
   "source": [
    "# Parse time column - it's already in YYYY-MM-DD format\n",
    "df['date'] = pd.to_datetime(df['time'], errors='coerce')\n",
    "\n",
    "# Check for parsing errors\n",
    "print(f\"Successfully parsed dates: {df['date'].notna().sum()} / {len(df)}\")\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "\n",
    "# Set frequency to Month Start for time series analysis\n",
    "df['period'] = df['date'].dt.to_period('M')\n",
    "\n",
    "print(f\"\\nUnique periods: {df['period'].nunique()}\")\n",
    "print(f\"Sample periods: {sorted(df['period'].unique())[:12]}\")\n",
    "\n",
    "# Display data structure\n",
    "print(f\"\\nData structure (long format - multiple rows per product):\")\n",
    "print(df[['product', 'date', 'period', 'sold/m', 'revenue/month', 'top-level_category']].head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca61ed58",
   "metadata": {},
   "source": [
    "## 6. Product-Level Processing: Truncation and Filtering\n",
    "**Step A:** Filter products marked as suitable for seasonal analysis  \n",
    "**Step B:** Create time series per product (pivot from long to wide format)  \n",
    "**Step C:** Truncate pre-listing NaNs and filter by duration (≥12 months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4ce52ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Products marked suitable for seasonal analysis: 31715\n",
      "\n",
      "Pivoted data shape: (31715, 45)\n",
      "Products: 31715, Time periods: 45\n",
      "\n",
      "Time periods: [Period('2022-03', 'M'), Period('2022-04', 'M'), Period('2022-05', 'M'), Period('2022-06', 'M'), Period('2022-07', 'M'), Period('2022-08', 'M'), Period('2022-09', 'M'), Period('2022-10', 'M'), Period('2022-11', 'M'), Period('2022-12', 'M'), Period('2023-01', 'M'), Period('2023-02', 'M')]\n",
      "\n",
      "Pivoted data shape: (31715, 45)\n",
      "Products: 31715, Time periods: 45\n",
      "\n",
      "Time periods: [Period('2022-03', 'M'), Period('2022-04', 'M'), Period('2022-05', 'M'), Period('2022-06', 'M'), Period('2022-07', 'M'), Period('2022-08', 'M'), Period('2022-09', 'M'), Period('2022-10', 'M'), Period('2022-11', 'M'), Period('2022-12', 'M'), Period('2023-01', 'M'), Period('2023-02', 'M')]\n",
      "\n",
      "Total products after pivot: 31715\n",
      "Products with ≥12 months data: 31710\n",
      "Filtered out: 5 products\n",
      "\n",
      "Total products after pivot: 31715\n",
      "Products with ≥12 months data: 31710\n",
      "Filtered out: 5 products\n"
     ]
    }
   ],
   "source": [
    "# Step A: Filter products suitable for seasonal analysis\n",
    "if 'suitable_for_seasonal_analysis' in df.columns:\n",
    "    df_filtered = df[df['suitable_for_seasonal_analysis'] == True].copy()\n",
    "    print(f\"Products marked suitable for seasonal analysis: {df_filtered['product'].nunique()}\")\n",
    "else:\n",
    "    df_filtered = df.copy()\n",
    "    print(\"No 'suitable_for_seasonal_analysis' column found, using all products\")\n",
    "\n",
    "# Step B: Pivot data to create time series per product\n",
    "# Primary metric: sold/m (units sold per month)\n",
    "pivot_sold = df_filtered.pivot_table(\n",
    "    index='product',\n",
    "    columns='period',\n",
    "    values='sold/m',\n",
    "    aggfunc='first'  # Use first value if duplicates\n",
    ")\n",
    "\n",
    "# Secondary metric: revenue/month (for validation)\n",
    "pivot_revenue = df_filtered.pivot_table(\n",
    "    index='product',\n",
    "    columns='period',\n",
    "    values='revenue/month',\n",
    "    aggfunc='first'\n",
    ")\n",
    "\n",
    "# Get category mapping\n",
    "product_category = df_filtered.groupby('product')['top-level_category'].first()\n",
    "\n",
    "print(f\"\\nPivoted data shape: {pivot_sold.shape}\")\n",
    "print(f\"Products: {len(pivot_sold)}, Time periods: {len(pivot_sold.columns)}\")\n",
    "print(f\"\\nTime periods: {pivot_sold.columns.tolist()[:12]}\")\n",
    "\n",
    "# Step C: Process each product - truncate and filter\n",
    "processed_products = []\n",
    "\n",
    "for product_id in pivot_sold.index:\n",
    "    # Get time series data\n",
    "    ts_data = pivot_sold.loc[product_id].values\n",
    "    ts_dates = pivot_sold.columns\n",
    "    \n",
    "    # Find first valid (non-NaN) index (listing date)\n",
    "    valid_indices = np.where(~pd.isna(ts_data))[0]\n",
    "    \n",
    "    if len(valid_indices) == 0:\n",
    "        continue  # Skip products with no data\n",
    "    \n",
    "    first_valid_idx = valid_indices[0]\n",
    "    \n",
    "    # Truncate pre-listing NaNs\n",
    "    truncated_data = ts_data[first_valid_idx:]\n",
    "    truncated_dates = ts_dates[first_valid_idx:]\n",
    "    \n",
    "    # Filter by duration (≥12 months)\n",
    "    if len(truncated_data) < 12:\n",
    "        continue\n",
    "    \n",
    "    # Store processed product\n",
    "    processed_products.append({\n",
    "        'product_id': product_id,\n",
    "        'category': product_category.get(product_id, 'Unknown'),\n",
    "        'time_series': truncated_data,\n",
    "        'dates': truncated_dates\n",
    "    })\n",
    "\n",
    "print(f\"\\nTotal products after pivot: {len(pivot_sold)}\")\n",
    "print(f\"Products with ≥12 months data: {len(processed_products)}\")\n",
    "print(f\"Filtered out: {len(pivot_sold) - len(processed_products)} products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5663467e",
   "metadata": {},
   "source": [
    "## 7. Category-Level Aggregation (Market Analysis)\n",
    "Sum sales by category to create category-level demand profiles for broad market trend analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ec896176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories found: 30\n",
      "\n",
      "Category summary:\n",
      "  Fashion Accessories: 1471 products, 45 months, Total sales: 3617350563\n",
      "  Stationery: 2013 products, 45 months, Total sales: 68900797\n",
      "  Beauty: 3301 products, 45 months, Total sales: 113272671\n",
      "  Home & Living: 7430 products, 45 months, Total sales: 255885372\n",
      "  Motorcycles: 678 products, 45 months, Total sales: 14355980\n",
      "  Sports & Outdoors: 1070 products, 45 months, Total sales: 28213870\n",
      "  Women Clothes: 2132 products, 45 months, Total sales: 46957397\n",
      "  Mom & Baby: 1260 products, 45 months, Total sales: 69009790\n",
      "  Hobbies & Collections: 521 products, 45 months, Total sales: 11869135\n",
      "  Home Appliances: 935 products, 45 months, Total sales: 13102116\n",
      "  Pets: 1021 products, 45 months, Total sales: 32709656\n",
      "  Mobile & Gadgets: 2025 products, 45 months, Total sales: 38357822\n",
      "  Automobiles: 595 products, 45 months, Total sales: 8270098\n",
      "  Baby & Kids Fashion: 1073 products, 45 months, Total sales: 20217219\n",
      "  Health: 1247 products, 45 months, Total sales: 48453045\n",
      "  Men Clothes: 899 products, 45 months, Total sales: 22818965\n",
      "  Watches: 123 products, 45 months, Total sales: 2154850\n",
      "  Men Bags: 139 products, 45 months, Total sales: 2512713\n",
      "  Women Shoes: 647 products, 45 months, Total sales: 11226637\n",
      "  Women Bags: 608 products, 45 months, Total sales: 14844614\n",
      "  Food & Beverages: 777 products, 45 months, Total sales: 13982091\n",
      "  Travel & Luggage: 317 products, 45 months, Total sales: 8368018\n",
      "  Gaming & Consoles: 49 products, 45 months, Total sales: 1126565\n",
      "  Audio: 446 products, 45 months, Total sales: 8498444\n",
      "  Computers & Accessories: 375 products, 45 months, Total sales: 3769238\n",
      "  Books & Magazines: 90 products, 45 months, Total sales: 1357858\n",
      "  Men Shoes: 291 products, 45 months, Total sales: 5854736\n",
      "  Cameras & Drones: 159 products, 45 months, Total sales: 1291732\n",
      "  Tickets, Vouchers & Services: 10 products, 45 months, Total sales: 11406012\n",
      "  Muslim Fashion: 8 products, 45 months, Total sales: 79743\n"
     ]
    }
   ],
   "source": [
    "# Group products by category and aggregate sales\n",
    "category_aggregates = {}\n",
    "\n",
    "for product in processed_products:\n",
    "    category = product['category']\n",
    "    dates = product['dates']\n",
    "    sales = product['time_series']\n",
    "    \n",
    "    if category not in category_aggregates:\n",
    "        # Initialize category with full date range\n",
    "        # Find the earliest and latest dates across all products in category\n",
    "        category_aggregates[category] = {\n",
    "            'products': [],\n",
    "            'dates_list': []\n",
    "        }\n",
    "    \n",
    "    category_aggregates[category]['products'].append({\n",
    "        'dates': dates,\n",
    "        'sales': sales\n",
    "    })\n",
    "\n",
    "# Aggregate sales for each category\n",
    "for category in category_aggregates:\n",
    "    # Find common date range\n",
    "    all_dates = []\n",
    "    for prod in category_aggregates[category]['products']:\n",
    "        all_dates.extend(prod['dates'].tolist())\n",
    "    \n",
    "    unique_dates = sorted(set(all_dates))\n",
    "    \n",
    "    # Sum sales across products for each date\n",
    "    aggregated_sales = np.zeros(len(unique_dates))\n",
    "    \n",
    "    for prod in category_aggregates[category]['products']:\n",
    "        for i, date in enumerate(unique_dates):\n",
    "            if date in prod['dates']:\n",
    "                idx = prod['dates'].tolist().index(date)\n",
    "                if not pd.isna(prod['sales'][idx]):\n",
    "                    aggregated_sales[i] += prod['sales'][idx]\n",
    "    \n",
    "    category_aggregates[category]['dates'] = pd.PeriodIndex(unique_dates)\n",
    "    category_aggregates[category]['sales'] = aggregated_sales\n",
    "\n",
    "print(f\"Categories found: {len(category_aggregates)}\")\n",
    "print(f\"\\nCategory summary:\")\n",
    "for cat, data in category_aggregates.items():\n",
    "    valid_sales = data['sales'][~pd.isna(data['sales'])]\n",
    "    print(f\"  {cat}: {len(data['products'])} products, {len(data['dates'])} months, \"\n",
    "          f\"Total sales: {np.nansum(valid_sales):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d23fd8a",
   "metadata": {},
   "source": [
    "## 8. Seasonal Decomposition - Product Level\n",
    "Apply additive decomposition (Y = T + S + R) to each product with ≥24 months of data (minimum for reliable seasonal decomposition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "51dbad08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully decomposed 25228 products (≥24 months)\n"
     ]
    }
   ],
   "source": [
    "# Apply seasonal decomposition to products with ≥24 months\n",
    "product_decompositions = []\n",
    "\n",
    "for product in processed_products:\n",
    "    # Require at least 24 months for reliable decomposition\n",
    "    if len(product['time_series']) < 24:\n",
    "        continue\n",
    "    \n",
    "    # Create time series with period index, convert to timestamp for decomposition\n",
    "    dates_timestamp = product['dates'].to_timestamp()\n",
    "    \n",
    "    ts = pd.Series(\n",
    "        product['time_series'],\n",
    "        index=dates_timestamp\n",
    "    )\n",
    "    \n",
    "    # Handle NaN values within the series (interpolate)\n",
    "    ts_filled = ts.interpolate(method='linear', limit_direction='both')\n",
    "    \n",
    "    # Check if we have enough valid data after interpolation\n",
    "    if ts_filled.isna().sum() > len(ts_filled) * 0.3:  # Skip if >30% NaN\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Apply additive decomposition with 12-month period\n",
    "        decomposition = seasonal_decompose(\n",
    "            ts_filled,\n",
    "            model='additive',\n",
    "            period=12,\n",
    "            extrapolate_trend='freq'\n",
    "        )\n",
    "        \n",
    "        product_decompositions.append({\n",
    "            'product_id': product['product_id'],\n",
    "            'category': product['category'],\n",
    "            'original': ts_filled,\n",
    "            'trend': decomposition.trend,\n",
    "            'seasonal': decomposition.seasonal,\n",
    "            'residual': decomposition.resid,\n",
    "            'dates': dates_timestamp\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Decomposition failed for product {product['product_id']}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"Successfully decomposed {len(product_decompositions)} products (≥24 months)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af40e55",
   "metadata": {},
   "source": [
    "## 9. Seasonal Decomposition - Category Level\n",
    "Apply decomposition to aggregated category sales for market-level trend analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "91d627b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully decomposed 30 categories (≥24 months)\n"
     ]
    }
   ],
   "source": [
    "# Apply decomposition to category aggregates with ≥24 months\n",
    "category_decompositions = []\n",
    "\n",
    "for category, cat_data in category_aggregates.items():\n",
    "    # Require at least 24 months for reliable decomposition\n",
    "    if len(cat_data['sales']) < 24:\n",
    "        continue\n",
    "    \n",
    "    # Create time series with timestamp index\n",
    "    dates_timestamp = cat_data['dates'].to_timestamp()\n",
    "    \n",
    "    ts = pd.Series(\n",
    "        cat_data['sales'],\n",
    "        index=dates_timestamp\n",
    "    )\n",
    "    \n",
    "    # Handle NaN values (interpolate)\n",
    "    ts_filled = ts.interpolate(method='linear', limit_direction='both')\n",
    "    \n",
    "    # Check if we have enough valid data\n",
    "    if ts_filled.isna().sum() > len(ts_filled) * 0.3:  # Skip if >30% NaN\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Apply additive decomposition with 12-month period\n",
    "        decomposition = seasonal_decompose(\n",
    "            ts_filled,\n",
    "            model='additive',\n",
    "            period=12,\n",
    "            extrapolate_trend='freq'\n",
    "        )\n",
    "        \n",
    "        category_decompositions.append({\n",
    "            'category': category,\n",
    "            'original': ts_filled,\n",
    "            'trend': decomposition.trend,\n",
    "            'seasonal': decomposition.seasonal,\n",
    "            'residual': decomposition.resid,\n",
    "            'dates': dates_timestamp\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Decomposition failed for category {category}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"Successfully decomposed {len(category_decompositions)} categories (≥24 months)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba07301c",
   "metadata": {},
   "source": [
    "## 10. Extract Metrics for Product-Level Analysis\n",
    "Calculate key metrics: mean monthly sales, seasonal amplitude, trend slope, and residual anomalies (z-scores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f8138f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted metrics for 25228 products\n",
      "\n",
      "Metrics summary:\n",
      "       mean_monthly_sales  seasonal_amplitude   trend_slope  \\\n",
      "count        25228.000000        25228.000000  25228.000000   \n",
      "mean           746.077101           18.618447     -9.261040   \n",
      "std           5114.425114           36.497179    248.146643   \n",
      "min              0.000000            0.000000 -27838.214143   \n",
      "25%             60.234163            4.885611    -13.321977   \n",
      "50%            148.400000            8.894620     -2.211501   \n",
      "75%            437.450000           16.234967      4.237078   \n",
      "max         355084.400000          300.000000   7926.994035   \n",
      "\n",
      "       max_residual_zscore  \n",
      "count         25228.000000  \n",
      "mean              4.178137  \n",
      "std               1.287364  \n",
      "min               0.000000  \n",
      "25%               3.244320  \n",
      "50%               4.458292  \n",
      "75%               5.318657  \n",
      "max               5.740880  \n"
     ]
    }
   ],
   "source": [
    "# Extract metrics for each decomposed product\n",
    "product_metrics = []\n",
    "\n",
    "for decomp in product_decompositions:\n",
    "    # Calculate mean monthly sales\n",
    "    mean_monthly_sales = decomp['original'].mean()\n",
    "    \n",
    "    # Calculate seasonal amplitude: (Max(S) - Min(S)) / Mean(T)\n",
    "    seasonal_max = decomp['seasonal'].max()\n",
    "    seasonal_min = decomp['seasonal'].min()\n",
    "    trend_mean = decomp['trend'].mean()\n",
    "    seasonal_amplitude = (seasonal_max - seasonal_min) / trend_mean if trend_mean != 0 else 0\n",
    "    \n",
    "    # Calculate trend slope using linear regression\n",
    "    x = np.arange(len(decomp['trend']))\n",
    "    y = decomp['trend'].values\n",
    "    valid_mask = ~np.isnan(y)\n",
    "    if valid_mask.sum() > 1:\n",
    "        trend_slope, _ = np.polyfit(x[valid_mask], y[valid_mask], 1)\n",
    "    else:\n",
    "        trend_slope = 0\n",
    "    \n",
    "    # Calculate max residual z-score\n",
    "    residuals = decomp['residual'].dropna()\n",
    "    if len(residuals) > 0 and residuals.std() != 0:\n",
    "        residual_zscores = np.abs((residuals - residuals.mean()) / residuals.std())\n",
    "        max_residual_zscore = residual_zscores.max()\n",
    "    else:\n",
    "        max_residual_zscore = 0\n",
    "    \n",
    "    product_metrics.append({\n",
    "        'product': decomp['product_id'],\n",
    "        'category': decomp['category'],\n",
    "        'mean_monthly_sales': mean_monthly_sales,\n",
    "        'seasonal_amplitude': seasonal_amplitude,\n",
    "        'trend_slope': trend_slope,\n",
    "        'max_residual_zscore': max_residual_zscore\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "metrics_df = pd.DataFrame(product_metrics)\n",
    "\n",
    "print(f\"Extracted metrics for {len(metrics_df)} products\")\n",
    "print(f\"\\nMetrics summary:\")\n",
    "print(metrics_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dcbc10",
   "metadata": {},
   "source": [
    "## 11. Export Results to CSV\n",
    "Export the decomposition metrics to CSV for use in subsequent analysis (Task 2.2.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5b035a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics exported to 'seasonal_trend/seasonal_decomposition_results.csv'\n",
      "Total rows: 25228\n",
      "\n",
      "Columns: ['product', 'category', 'mean_monthly_sales', 'seasonal_amplitude', 'trend_slope', 'max_residual_zscore']\n"
     ]
    }
   ],
   "source": [
    "# Export metrics to CSV\n",
    "output_filename = 'seasonal_trend/seasonal_decomposition_results.csv'\n",
    "metrics_df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"Metrics exported to '{output_filename}'\")\n",
    "print(f\"Total rows: {len(metrics_df)}\")\n",
    "print(f\"\\nColumns: {list(metrics_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb583de6",
   "metadata": {},
   "source": [
    "## 12. Visualization - Strategic Sample Selection\n",
    "Identify products for visualization based on highest seasonal amplitude and residual anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4292b59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategic Samples Selected:\n",
      "\n",
      "Top 10 by Seasonal Amplitude:\n",
      "                                                 product          category  \\\n",
      "1798   20 Color Eyeshadow Palette Sequins Glitter Pea...            Beauty   \n",
      "5663   Black Suit Dust Cover Bag Formal Dress Storage...     Home & Living   \n",
      "5710   BlueTooth Speaker With Phone Stand 5 In 1 Mobi...  Mobile & Gadgets   \n",
      "6915            Cavendish & Harvey Clear Mint Drops 200g  Food & Beverages   \n",
      "7703   Cute Felt Cartoon Bag – Mommy Gift Tote for Hu...        Women Bags   \n",
      "9181   FUTURE  Tote Bag, PVC Large Capacity Cosmetics...        Women Bags   \n",
      "11287  Irregular Mirror Acrylic Mirror Wood Base Make...            Beauty   \n",
      "13554  MICHAELA Chic Flap Card Holder Minimalist Slim...        Women Bags   \n",
      "16137  PER METER Multipurpose Clear Level Hose Good Q...     Home & Living   \n",
      "1974   20Pcs Furniture Chair Rubber Foot Cover 19mm R...     Home & Living   \n",
      "\n",
      "       seasonal_amplitude  \n",
      "1798                300.0  \n",
      "5663                300.0  \n",
      "5710                300.0  \n",
      "6915                300.0  \n",
      "7703                300.0  \n",
      "9181                300.0  \n",
      "11287               300.0  \n",
      "13554               300.0  \n",
      "16137               300.0  \n",
      "1974                300.0  \n",
      "\n",
      "Top 10 by Residual Anomalies:\n",
      "                                                 product  \\\n",
      "6975   Cetaphil Gentle Skin Cleanser 250ml [For Sensi...   \n",
      "16229  PINKFLASH Soft Smooth Pigmented Natural Natura...   \n",
      "22863  [1PCS/4PCS] Unibest Makapal Stainless Steel Fo...   \n",
      "48     #31 - #72 BOSNY 100% Acrylic Spray Paint Assor...   \n",
      "378    1 Meters Speargun Rubber spearfishing band tub...   \n",
      "447     1 Piece Tempered Glass Screen Protector for iPad   \n",
      "1119   12 Pieces Hand and Face Towel Set – Highly Abs...   \n",
      "1185   12Pc/Set Colorful Double Layer 3D Magnetic But...   \n",
      "1394   18650 Lithium Battery 3.7V 3000mAh Rechargeabl...   \n",
      "1776   2.1M Rectangular Inflatable Family Swimming Po...   \n",
      "\n",
      "                    category  max_residual_zscore  \n",
      "6975                  Beauty             5.740880  \n",
      "16229                 Beauty             5.739583  \n",
      "22863          Home & Living             5.739583  \n",
      "48             Home & Living             5.739583  \n",
      "378        Sports & Outdoors             5.739583  \n",
      "447         Mobile & Gadgets             5.739583  \n",
      "1119           Home & Living             5.739583  \n",
      "1185   Hobbies & Collections             5.739583  \n",
      "1394         Home Appliances             5.739583  \n",
      "1776              Mom & Baby             5.739583  \n",
      "\n",
      "Category Representatives (30 categories):\n",
      "                                                 product  \\\n",
      "22787  Zeus 6D Surround Sound Bass Earphones W/ Micro...   \n",
      "1667   1pcs Car Shock Absorber Gasket Car Door Sound ...   \n",
      "18771  Samara PAJAMA TERNO SET for Kids | Cotton Span...   \n",
      "2795   4D Mask 10pcs/Pack Face-lifting Butterfly Mask...   \n",
      "15863  Original Atomic Habits by James Clear 100% Eng...   \n",
      "9064   FDA Silica Gel Desiccant for Food, Leather, Ba...   \n",
      "8910   Epson 003 Inks for L1210,L1250,L3210,L3216,L32...   \n",
      "6571   CTW Fashion 3D 10Pcs Korea Style Face Mask Aes...   \n",
      "6360                    COD Kimberly gummy 250g and 500g   \n",
      "1221   12pc Gaming Finger Sleeves - 12-Piece Mixed Co...   \n",
      "24423  《BiuBiu》3D Face Mask 10Pcs Korean 3D Face-lift...   \n",
      "14181  Milk Yarn 5-Ply Smooth Fiber Knitting Wool Cro...   \n",
      "757    100Pcs Powder-Free Vinyl Nitrile Gloves - Safe...   \n",
      "22741  ZH Table Clip Fan 4/5 Blades Mini Home Electri...   \n",
      "14607  Mumu 1038 Expansion Card Holder Large-Capacity...   \n",
      "15371  Non-Slip Silicone Sports Socks for Basketball ...   \n",
      "19141   Shoes Shield Anti Wrinkle Shoes Crease Protector   \n",
      "20488  Tempered Glass For Realme C15 C11 C12 C21y C25...   \n",
      "595    10 Pack Facial Tissue Paper Towel 10 Pack Wipe...   \n",
      "12475  Koyo Bearings JAPAN All size 6200/6201/6202/62...   \n",
      "23446          cotton Sarong  size 120/140 cm for unisex   \n",
      "3350          6L Bentonite Tofu Cat Litter - Multi-Blend   \n",
      "8474   Durable Folding Umbrella for All Weather Condi...   \n",
      "24381                      ❀◇ Receipt Resibo With Carbon   \n",
      "10073                                      Globe Load 20   \n",
      "19237  Simmi #1PCS Waterproof Transparent Clothes Und...   \n",
      "23257    [jden] #159 square stainless steal watch unisex   \n",
      "9732   Frosted Plastic PVC Transparent Bags Souvenir ...   \n",
      "9803   GBra Korean Design Wonderbra Sexy Comfortable ...   \n",
      "19543  Someday 12/30/80Pcs Orginal Wipes For Sneakers...   \n",
      "\n",
      "                           category  mean_monthly_sales  \n",
      "22787                         Audio        33968.755556  \n",
      "1667                    Automobiles        10014.525000  \n",
      "18771           Baby & Kids Fashion        48554.044444  \n",
      "2795                         Beauty       268383.022222  \n",
      "15863             Books & Magazines         3256.088889  \n",
      "9064               Cameras & Drones         2626.911111  \n",
      "8910        Computers & Accessories         2812.311111  \n",
      "6571            Fashion Accessories        66417.977778  \n",
      "6360               Food & Beverages        16338.733333  \n",
      "1221              Gaming & Consoles         9181.392857  \n",
      "24423                        Health       209358.600000  \n",
      "14181         Hobbies & Collections        27035.047619  \n",
      "757                   Home & Living       355084.400000  \n",
      "22741               Home Appliances        15998.111111  \n",
      "14607                      Men Bags         7591.790698  \n",
      "15371                   Men Clothes        25075.041667  \n",
      "19141                     Men Shoes        23159.133333  \n",
      "20488              Mobile & Gadgets        32972.822222  \n",
      "595                      Mom & Baby       329682.535714  \n",
      "12475                   Motorcycles        42530.777778  \n",
      "23446                Muslim Fashion          446.466667  \n",
      "3350                           Pets        99324.600000  \n",
      "8474              Sports & Outdoors        36550.466667  \n",
      "24381                    Stationery       176429.232558  \n",
      "10073  Tickets, Vouchers & Services        91899.888889  \n",
      "19237              Travel & Luggage        11111.111111  \n",
      "23257                       Watches         7942.044444  \n",
      "9732                     Women Bags        59162.035714  \n",
      "9803                  Women Clothes        24735.333333  \n",
      "19543                   Women Shoes        10617.763158  \n"
     ]
    }
   ],
   "source": [
    "# Select strategic samples for visualization\n",
    "# Top 10 by seasonal amplitude\n",
    "top_seasonal = metrics_df.nlargest(10, 'seasonal_amplitude')\n",
    "\n",
    "# Top 10 by max residual z-score\n",
    "top_anomaly = metrics_df.nlargest(10, 'max_residual_zscore')\n",
    "\n",
    "# One representative per category (highest mean sales)\n",
    "category_reps = metrics_df.loc[metrics_df.groupby('category')['mean_monthly_sales'].idxmax()]\n",
    "\n",
    "print(\"Strategic Samples Selected:\")\n",
    "print(f\"\\nTop 10 by Seasonal Amplitude:\")\n",
    "print(top_seasonal[['product', 'category', 'seasonal_amplitude']])\n",
    "print(f\"\\nTop 10 by Residual Anomalies:\")\n",
    "print(top_anomaly[['product', 'category', 'max_residual_zscore']])\n",
    "print(f\"\\nCategory Representatives ({len(category_reps)} categories):\")\n",
    "print(category_reps[['product', 'category', 'mean_monthly_sales']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfeb9bd",
   "metadata": {},
   "source": [
    "## 13. Visualize Decomposition for Top Seasonal Products\n",
    "Plot decomposition components (Trend, Seasonal, Residual) for products with highest seasonal amplitude.\n",
    "Each product gets its own separate graph saved in a dedicated directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3d27c9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 10 seasonal decomposition plots to 'seasonal_trend/visualizations/top_seasonal_products/'\n",
      "Files saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create directory for top seasonal products\n",
    "output_dir = 'seasonal_trend/visualizations/top_seasonal_products'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Visualize decomposition for top seasonal products (individual plots)\n",
    "top_seasonal_ids = top_seasonal['product'].head(10).values\n",
    "\n",
    "for product_id in top_seasonal_ids:\n",
    "    # Find decomposition for this product\n",
    "    decomp = next((d for d in product_decompositions if d['product_id'] == product_id), None)\n",
    "    \n",
    "    if decomp is None:\n",
    "        continue\n",
    "    \n",
    "    # Create a 2x2 subplot for this product\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(f\"Seasonal Decomposition: {product_id}\", fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot original series\n",
    "    axes[0, 0].plot(decomp['dates'], decomp['original'], color='blue', linewidth=2)\n",
    "    axes[0, 0].set_title('Original Series', fontsize=12)\n",
    "    axes[0, 0].set_ylabel('Sales')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot trend\n",
    "    axes[0, 1].plot(decomp['dates'], decomp['trend'], color='green', linewidth=2)\n",
    "    axes[0, 1].set_title('Trend Component', fontsize=12)\n",
    "    axes[0, 1].set_ylabel('Trend')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot seasonal\n",
    "    axes[1, 0].plot(decomp['dates'], decomp['seasonal'], color='orange', linewidth=2)\n",
    "    axes[1, 0].set_title('Seasonal Component', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Seasonal')\n",
    "    axes[1, 0].axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot residual\n",
    "    axes[1, 1].plot(decomp['dates'], decomp['residual'], color='red', linewidth=2)\n",
    "    axes[1, 1].set_title('Residual Component', fontsize=12)\n",
    "    axes[1, 1].set_ylabel('Residual')\n",
    "    axes[1, 1].axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save with sanitized filename\n",
    "    safe_filename = \"\".join(c for c in product_id if c.isalnum() or c in (' ', '-', '_')).rstrip()\n",
    "    safe_filename = safe_filename.replace(' ', '_')[:100]  # Limit length\n",
    "    filepath = os.path.join(output_dir, f'{safe_filename}.png')\n",
    "    plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "print(f\"Saved {len(top_seasonal_ids)} seasonal decomposition plots to '{output_dir}/'\")\n",
    "print(f\"Files saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8fbbf2",
   "metadata": {},
   "source": [
    "## 14. Visualize Decomposition for Top Anomaly Products\n",
    "Plot decomposition with highlighted anomalies for products with highest residual z-scores.\n",
    "Each product gets its own separate graph saved in a dedicated directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2318058d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 10 anomaly decomposition plots to 'seasonal_trend/visualizations/top_anomaly_products/'\n",
      "Files saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create directory for top anomaly products\n",
    "output_dir = 'seasonal_trend/visualizations/top_anomaly_products'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Visualize decomposition for top anomaly products (individual plots)\n",
    "top_anomaly_ids = top_anomaly['product'].head(10).values\n",
    "\n",
    "for product_id in top_anomaly_ids:\n",
    "    # Find decomposition for this product\n",
    "    decomp = next((d for d in product_decompositions if d['product_id'] == product_id), None)\n",
    "    \n",
    "    if decomp is None:\n",
    "        continue\n",
    "    \n",
    "    # Create a 2x2 subplot for this product\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(f\"Seasonal Decomposition with Anomalies: {product_id}\", fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot original series\n",
    "    axes[0, 0].plot(decomp['dates'], decomp['original'], color='blue', linewidth=2)\n",
    "    axes[0, 0].set_title('Original Series', fontsize=12)\n",
    "    axes[0, 0].set_ylabel('Sales')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot trend\n",
    "    axes[0, 1].plot(decomp['dates'], decomp['trend'], color='green', linewidth=2)\n",
    "    axes[0, 1].set_title('Trend Component', fontsize=12)\n",
    "    axes[0, 1].set_ylabel('Trend')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot seasonal\n",
    "    axes[1, 0].plot(decomp['dates'], decomp['seasonal'], color='orange', linewidth=2)\n",
    "    axes[1, 0].set_title('Seasonal Component', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Seasonal')\n",
    "    axes[1, 0].axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot residual with highlighted anomalies\n",
    "    residuals = decomp['residual']\n",
    "    axes[1, 1].plot(decomp['dates'], residuals, color='red', linewidth=2, alpha=0.6)\n",
    "    \n",
    "    # Highlight anomalies (|z-score| > 2)\n",
    "    if residuals.std() != 0:\n",
    "        z_scores = np.abs((residuals - residuals.mean()) / residuals.std())\n",
    "        anomaly_mask = z_scores > 2\n",
    "        axes[1, 1].scatter(decomp['dates'][anomaly_mask], residuals[anomaly_mask], \n",
    "                          color='darkred', s=100, zorder=5, label='Anomalies (|z| > 2)', marker='*')\n",
    "        axes[1, 1].legend(loc='best')\n",
    "    \n",
    "    axes[1, 1].set_title('Residual Component with Anomalies', fontsize=12)\n",
    "    axes[1, 1].set_ylabel('Residual')\n",
    "    axes[1, 1].axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save with sanitized filename\n",
    "    safe_filename = \"\".join(c for c in product_id if c.isalnum() or c in (' ', '-', '_')).rstrip()\n",
    "    safe_filename = safe_filename.replace(' ', '_')[:100]\n",
    "    filepath = os.path.join(output_dir, f'{safe_filename}.png')\n",
    "    plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "print(f\"Saved {len(top_anomaly_ids)} anomaly decomposition plots to '{output_dir}/'\")\n",
    "print(f\"Files saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc6c66d",
   "metadata": {},
   "source": [
    "## 15. Visualize Category-Level Decomposition\n",
    "Plot category-level decompositions for market trend analysis.\n",
    "Each category gets its own separate graph saved in a dedicated directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5814b261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 30 category decomposition plots to 'seasonal_trend/visualizations/category_decompositions/'\n",
      "Files saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create directory for category-level decompositions\n",
    "output_dir = 'seasonal_trend/visualizations/category_decompositions'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Visualize category-level decompositions (individual plots)\n",
    "if len(category_decompositions) > 0:\n",
    "    for decomp in category_decompositions:\n",
    "        # Create a 2x2 subplot for this category\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle(f\"Category Decomposition: {decomp['category']}\", fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Plot original series\n",
    "        axes[0, 0].plot(decomp['dates'], decomp['original'], color='blue', linewidth=2)\n",
    "        axes[0, 0].set_title('Original Series (Total Sales)', fontsize=12)\n",
    "        axes[0, 0].set_ylabel('Total Sales')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Plot trend\n",
    "        axes[0, 1].plot(decomp['dates'], decomp['trend'], color='green', linewidth=2)\n",
    "        axes[0, 1].set_title('Trend Component', fontsize=12)\n",
    "        axes[0, 1].set_ylabel('Trend')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Plot seasonal\n",
    "        axes[1, 0].plot(decomp['dates'], decomp['seasonal'], color='orange', linewidth=2)\n",
    "        axes[1, 0].set_title('Seasonal Component', fontsize=12)\n",
    "        axes[1, 0].set_ylabel('Seasonal')\n",
    "        axes[1, 0].axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Plot residual\n",
    "        axes[1, 1].plot(decomp['dates'], decomp['residual'], color='red', linewidth=2, alpha=0.7)\n",
    "        axes[1, 1].set_title('Residual Component', fontsize=12)\n",
    "        axes[1, 1].set_ylabel('Residual')\n",
    "        axes[1, 1].axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save with sanitized filename\n",
    "        safe_filename = \"\".join(c for c in decomp['category'] if c.isalnum() or c in (' ', '-', '_')).rstrip()\n",
    "        safe_filename = safe_filename.replace(' ', '_')[:100]\n",
    "        filepath = os.path.join(output_dir, f'{safe_filename}.png')\n",
    "        plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    print(f\"Saved {len(category_decompositions)} category decomposition plots to '{output_dir}/'\")\n",
    "    print(f\"Files saved successfully!\")\n",
    "else:\n",
    "    print(\"No category decompositions available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1605a5ab",
   "metadata": {},
   "source": [
    "## 16. Seasonal Subseries Plot (Calendar View)\n",
    "Visualize seasonal patterns across years for representative products.\n",
    "Each product gets its own separate graph saved in a dedicated directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d5f2d9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 10 seasonal subseries plots to 'seasonal_trend/visualizations/seasonal_subseries/'\n",
      "Files saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create directory for seasonal subseries plots\n",
    "output_dir = 'seasonal_trend/visualizations/seasonal_subseries'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Create seasonal subseries plots for top seasonal products\n",
    "def plot_seasonal_subseries(decomp, title, output_path):\n",
    "    \"\"\"Plot seasonal pattern by month across years\"\"\"\n",
    "    df_plot = pd.DataFrame({\n",
    "        'date': decomp['dates'],\n",
    "        'value': decomp['original'].values\n",
    "    })\n",
    "    df_plot['year'] = df_plot['date'].dt.year\n",
    "    df_plot['month'] = df_plot['date'].dt.month\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    # Plot each year as a separate line\n",
    "    for year in sorted(df_plot['year'].unique()):\n",
    "        year_data = df_plot[df_plot['year'] == year]\n",
    "        ax.plot(year_data['month'], year_data['value'], \n",
    "               marker='o', label=str(year), linewidth=2, markersize=6)\n",
    "    \n",
    "    ax.set_xlabel('Month', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Sales', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.set_xticks(range(1, 13))\n",
    "    ax.set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                        'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "    ax.legend(title='Year', bbox_to_anchor=(1.05, 1), loc='upper left', frameon=True)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Plot for top seasonal products\n",
    "top_for_subseries = top_seasonal['product'].head(10).values\n",
    "\n",
    "for product_id in top_for_subseries:\n",
    "    decomp = next((d for d in product_decompositions if d['product_id'] == product_id), None)\n",
    "    if decomp:\n",
    "        # Sanitize filename\n",
    "        safe_filename = \"\".join(c for c in product_id if c.isalnum() or c in (' ', '-', '_')).rstrip()\n",
    "        safe_filename = safe_filename.replace(' ', '_')[:100]\n",
    "        filepath = os.path.join(output_dir, f'{safe_filename}.png')\n",
    "        \n",
    "        plot_seasonal_subseries(decomp, f\"Seasonal Subseries - {product_id}\", filepath)\n",
    "\n",
    "print(f\"Saved {len(top_for_subseries)} seasonal subseries plots to '{output_dir}/'\")\n",
    "print(f\"Files saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c00df0",
   "metadata": {},
   "source": [
    "## 17. Summary and Interpretation\n",
    "Review key findings from the seasonal decomposition analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8d1e7a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SEASONAL DECOMPOSITION ANALYSIS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "1. DATA PROCESSING:\n",
      "   - Total products in dataset: 1510901\n",
      "   - Products with ≥12 months: 31710\n",
      "   - Products decomposed (≥24 months): 25228\n",
      "   - Categories analyzed: 30\n",
      "\n",
      "2. TOP PRODUCTS BY SEASONAL AMPLITUDE:\n",
      "   (Strong seasonal patterns - inventory optimization opportunities)\n",
      "   20 Color Eyeshadow Palette Sequins Glitter Pearles | Amplitude: 300.000\n",
      "   Black Suit Dust Cover Bag Formal Dress Storage Zip | Amplitude: 300.000\n",
      "   BlueTooth Speaker With Phone Stand 5 In 1 Mobile P | Amplitude: 300.000\n",
      "   Cavendish & Harvey Clear Mint Drops 200g           | Amplitude: 300.000\n",
      "   Cute Felt Cartoon Bag – Mommy Gift Tote for Hundre | Amplitude: 300.000\n",
      "\n",
      "3. TOP PRODUCTS BY TREND SLOPE:\n",
      "   (Emerging interest - growing demand)\n",
      "   ❀◇ Receipt Resibo With Carbon                      | Slope: 7926.994\n",
      "   100Pcs Powder-Free Vinyl Nitrile Gloves - Safe and | Slope: 6255.565\n",
      "   10 Pack Facial Tissue Paper Towel 10 Pack Wipes fo | Slope: 5739.238\n",
      "   《BiuBiu》3D Face Mask 10Pcs Korean 3D Face-lifting  | Slope: 5557.271\n",
      "   Sumikko 50 PCS  diaper for Baby Unisex Ultra thin  | Slope: 4866.323\n",
      "\n",
      "4. TOP PRODUCTS BY RESIDUAL ANOMALIES:\n",
      "   (Viral events or short-term consumer surges)\n",
      "   Cetaphil Gentle Skin Cleanser 250ml [For Sensitive | Max Z-Score: 5.74\n",
      "   PINKFLASH Soft Smooth Pigmented Natural Natural 3D | Max Z-Score: 5.74\n",
      "   [1PCS/4PCS] Unibest Makapal Stainless Steel Food W | Max Z-Score: 5.74\n",
      "   #31 - #72 BOSNY 100% Acrylic Spray Paint Assorted  | Max Z-Score: 5.74\n",
      "   1 Meters Speargun Rubber spearfishing band tubes 1 | Max Z-Score: 5.74\n",
      "\n",
      "5. CATEGORY-LEVEL INSIGHTS:\n",
      "   Fashion Accessories            | Trend Slope: 5290092.39 | Seasonal Amp: 9.725\n",
      "   Stationery                     | Trend Slope: 46708.34 | Seasonal Amp: 2.272\n",
      "   Beauty                         | Trend Slope: -15015.94 | Seasonal Amp: 2.126\n",
      "   Home & Living                  | Trend Slope: 100033.97 | Seasonal Amp: 2.781\n",
      "   Motorcycles                    | Trend Slope:  9475.50 | Seasonal Amp: 1.412\n",
      "   Sports & Outdoors              | Trend Slope: 14526.45 | Seasonal Amp: 2.952\n",
      "   Women Clothes                  | Trend Slope: -1538.47 | Seasonal Amp: 1.009\n",
      "   Mom & Baby                     | Trend Slope: 15329.48 | Seasonal Amp: 6.980\n",
      "   Hobbies & Collections          | Trend Slope:  3767.91 | Seasonal Amp: 0.764\n",
      "   Home Appliances                | Trend Slope:  2246.39 | Seasonal Amp: 1.066\n",
      "   Pets                           | Trend Slope:  9343.83 | Seasonal Amp: 2.749\n",
      "   Mobile & Gadgets               | Trend Slope:  7986.12 | Seasonal Amp: 1.385\n",
      "   Automobiles                    | Trend Slope:  6419.67 | Seasonal Amp: 0.533\n",
      "   Baby & Kids Fashion            | Trend Slope:  2354.89 | Seasonal Amp: 1.282\n",
      "   Health                         | Trend Slope: -8311.40 | Seasonal Amp: 2.188\n",
      "   Men Clothes                    | Trend Slope:  3321.06 | Seasonal Amp: 1.163\n",
      "   Watches                        | Trend Slope:   400.72 | Seasonal Amp: 2.316\n",
      "   Men Bags                       | Trend Slope:   372.27 | Seasonal Amp: 2.251\n",
      "   Women Shoes                    | Trend Slope:   595.94 | Seasonal Amp: 0.922\n",
      "   Women Bags                     | Trend Slope:  2115.10 | Seasonal Amp: 4.551\n",
      "   Food & Beverages               | Trend Slope:  6115.04 | Seasonal Amp: 1.453\n",
      "   Travel & Luggage               | Trend Slope:  1879.86 | Seasonal Amp: 3.246\n",
      "   Gaming & Consoles              | Trend Slope:   245.06 | Seasonal Amp: 1.659\n",
      "   Audio                          | Trend Slope:  1239.66 | Seasonal Amp: 2.313\n",
      "   Computers & Accessories        | Trend Slope:   399.16 | Seasonal Amp: 1.305\n",
      "   Books & Magazines              | Trend Slope:   647.87 | Seasonal Amp: 1.184\n",
      "   Men Shoes                      | Trend Slope:  2358.40 | Seasonal Amp: 0.875\n",
      "   Cameras & Drones               | Trend Slope:    47.69 | Seasonal Amp: 2.039\n",
      "   Tickets, Vouchers & Services   | Trend Slope: -18021.89 | Seasonal Amp: 3.369\n",
      "   Muslim Fashion                 | Trend Slope:    58.54 | Seasonal Amp: 4.017\n",
      "\n",
      "6. OUTPUT FILES GENERATED:\n",
      "   - seasonal_trend/seasonal_decomposition_results.csv (metrics for 25228 products)\n",
      "   - seasonal_trend/visualizations/top_seasonal_products/ (individual product plots)\n",
      "   - seasonal_trend/visualizations/top_anomaly_products/ (individual anomaly plots)\n",
      "   - seasonal_trend/visualizations/category_decompositions/ (individual category plots)\n",
      "   - seasonal_trend/visualizations/seasonal_subseries/ (individual subseries plots)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate summary report\n",
    "print(\"=\"*80)\n",
    "print(\"SEASONAL DECOMPOSITION ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1. DATA PROCESSING:\")\n",
    "print(f\"   - Total products in dataset: {len(df)}\")\n",
    "print(f\"   - Products with ≥12 months: {len(processed_products)}\")\n",
    "print(f\"   - Products decomposed (≥24 months): {len(product_decompositions)}\")\n",
    "print(f\"   - Categories analyzed: {len(category_decompositions)}\")\n",
    "\n",
    "print(f\"\\n2. TOP PRODUCTS BY SEASONAL AMPLITUDE:\")\n",
    "print(f\"   (Strong seasonal patterns - inventory optimization opportunities)\")\n",
    "for i, row in top_seasonal.head(5).iterrows():\n",
    "    print(f\"   {row['product'][:50]:50s} | Amplitude: {row['seasonal_amplitude']:.3f}\")\n",
    "\n",
    "print(f\"\\n3. TOP PRODUCTS BY TREND SLOPE:\")\n",
    "top_trending = metrics_df.nlargest(5, 'trend_slope')\n",
    "print(f\"   (Emerging interest - growing demand)\")\n",
    "for i, row in top_trending.iterrows():\n",
    "    print(f\"   {row['product'][:50]:50s} | Slope: {row['trend_slope']:.3f}\")\n",
    "\n",
    "print(f\"\\n4. TOP PRODUCTS BY RESIDUAL ANOMALIES:\")\n",
    "print(f\"   (Viral events or short-term consumer surges)\")\n",
    "for i, row in top_anomaly.head(5).iterrows():\n",
    "    print(f\"   {row['product'][:50]:50s} | Max Z-Score: {row['max_residual_zscore']:.2f}\")\n",
    "\n",
    "print(f\"\\n5. CATEGORY-LEVEL INSIGHTS:\")\n",
    "for decomp in category_decompositions:\n",
    "    trend_slope_cat = np.polyfit(range(len(decomp['trend'])), \n",
    "                                  decomp['trend'].fillna(method='ffill').values, 1)[0]\n",
    "    seasonal_amp_cat = (decomp['seasonal'].max() - decomp['seasonal'].min()) / decomp['trend'].mean()\n",
    "    print(f\"   {decomp['category']:30s} | Trend Slope: {trend_slope_cat:8.2f} | \"\n",
    "          f\"Seasonal Amp: {seasonal_amp_cat:.3f}\")\n",
    "\n",
    "print(f\"\\n6. OUTPUT FILES GENERATED:\")\n",
    "print(f\"   - seasonal_trend/seasonal_decomposition_results.csv (metrics for {len(metrics_df)} products)\")\n",
    "print(f\"   - seasonal_trend/visualizations/top_seasonal_products/ (individual product plots)\")\n",
    "print(f\"   - seasonal_trend/visualizations/top_anomaly_products/ (individual anomaly plots)\")\n",
    "print(f\"   - seasonal_trend/visualizations/category_decompositions/ (individual category plots)\")\n",
    "print(f\"   - seasonal_trend/visualizations/seasonal_subseries/ (individual subseries plots)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
